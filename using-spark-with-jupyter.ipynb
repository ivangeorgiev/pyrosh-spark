{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark with Jupiter\n",
    "\n",
    "This document is created as Jupyter Notebook and is available in different formats through export:\n",
    "* [Using Spark with Jupyter - HTML](using-spark-with-jupyter.html)\n",
    "* [Using Spark with Jupyter - Markdown](using-spark-with-jupyter.md)\n",
    "* [Using Spark with Jupyter - Jupyter Notebook .ipynb](using-spark-with-jupyter.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Jupyter with Spark in Docker Container\n",
    "\n",
    "Once you install docker, start a Jupyter container with spark.\n",
    "\n",
    "```\n",
    "> docker run -d --rm -p 18888:8888 -e GRANT_SUDO=yes -v C:\\Sandbox\\notebooks:/home/jovyan --name notebook jupyter/all-spark-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Spark with Jupyter on Windows\n",
    "\n",
    "Following procedure helps setting up Spark with Jupyter notebook on Windows.\n",
    "\n",
    "1. Instal Java - JDK 1.8+\n",
    "2. Install Python 3\n",
    "3. Install Spark\n",
    "4. Download winutils.exe\n",
    "5. Set Environment Variables\n",
    "6. Verify Components\n",
    "7. Start Jupyter Notebook\n",
    "\n",
    "### 1. Install JDK 1.8\n",
    "\n",
    "Go to Oracle's Java site to download and install latest JDK.\n",
    "\n",
    "Will assume Java is installed at `C:\\Program Files\\Java\\jdk1.8.0_191 `\n",
    "\n",
    "\n",
    "\n",
    "### 2. Install Python 3.7\n",
    "\n",
    "Download and install Python from https://www.python.org/downloads/.\n",
    "\n",
    "Will assume Python is installed at `C:\\Python37`.\n",
    "\n",
    "### 3. Install Spark\n",
    "\n",
    "Download Spark from https://spark.apache.org/downloads.html:\n",
    "\n",
    "- Spark release: 2.4.0 (Nov 02 2018)\n",
    "- Package type: Pre-built for Apache Hadoop 2.7 and later\n",
    "- Download: [spark-2.4.0-bin-hadoop2.7.tgz](https://www.apache.org/dyn/closer.lua/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz) \n",
    "\n",
    "Extract Spark. Will assume Spark is extracted at: `C:\\apps\\spark-2.4.0-bin-hadoop2.7`\n",
    "\n",
    "Modify log4j configuration to reduce log activity. Copy log4j.properties.template to log4j.properties. Modify:\n",
    "\n",
    "```\n",
    "log4j.rootCategory=WARN, console\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4. Download winutils.exe\n",
    "\n",
    "Download winutils.exe (http://media.sundog-soft.com/Udemy/winutils.exe or https://github.com/steveloughran/winutils) and place it under `%SPARK_HOME%\\bin`.\n",
    "\n",
    "### 5. Install Jupyter\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 6. Set Environment Variables\n",
    "\n",
    "```bash\n",
    "set JAVA_HOME=C:\\Program Files\\Java\\jdk1.8.0_191\n",
    "set SPARK_HOME=C:\\apps\\spark-2.4.0-bin-hadoop2.7\n",
    "set HADOOP_HOME=%SPARK_HOME%\n",
    "set PATH=%PATH%;%SPARK_HOME%\\bin\n",
    "```\n",
    "\n",
    "Open `Control Panel` and in the search box type `environment variables`. Click the `Edit the system environment variables` link.\n",
    "\n",
    "- The System Properties dialog opens.\n",
    "- Click the `Environment Variables...` button\n",
    "- Add above variables. Suggest that you not use variable references, but specify the values fully. This way you avoid problems caused by unpredictable order of variable evaluation and assignment.\n",
    "\n",
    "### 6. Verify Components\n",
    "\n",
    "```bash\n",
    "# Verify Python\n",
    "> python --version\n",
    "Python 3.7.1\n",
    "\n",
    "# Verify Java\n",
    "> java -version\n",
    "java version \"1.8.0_191\"\n",
    "Java(TM) SE Runtime Environment (build 1.8.0_191-b12)\n",
    "Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Start `pyspark` and execute simple program\n",
    "\n",
    "```python\n",
    ">>> spark.range(5).toDF(\"num\").show()\n",
    "+---+\n",
    "|num|\n",
    "+---+\n",
    "|  0|\n",
    "|  1|\n",
    "|  2|\n",
    "|  3|\n",
    "|  4|\n",
    "+---+\n",
    "```\n",
    "\n",
    "Press `Ctrl-Z` to exit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Start Jupyter Notebook\n",
    "\n",
    "Create a Spark bootstrap script and place it in `spark.py` file:\n",
    "\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "def get_spark(appName = 'HelloWorld'):\n",
    "    spark_home = os.path.abspath(os.environ.get('SPARK_HOME', None))\n",
    "    spark_python = os.path.abspath(spark_home + '/python')\n",
    "    pyj4 = os.path.abspath(glob.glob(spark_python + '/lib/py4j*.zip')[0])\n",
    "    if (not spark_python in sys.path):\n",
    "        sys.path.append(spark_python)\n",
    "    if (not pyj4 in sys.path):\n",
    "        sys.path.append(pyj4)\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(appName).getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = get_spark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to your notebook directory (`C:\\Sandbox\\notebook`) and start Jupyter:\n",
    "\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Create a new Python 3 notebook and execute following into a cell:\n",
    "\n",
    "```python\n",
    "%run spark.py\n",
    "```\n",
    "\n",
    "Inspect the `spark` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LYOGA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Hello World</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x281af449e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a new cell you can run simple Spark program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|num|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(5).toDF('num').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `findspark` Package\n",
    "\n",
    "`findspark` package provides `findspark.init()` function to make pyspark importable as a regular library. \n",
    "\n",
    "For more information on the package, see the [findspark github](https://github.com/minrk/findspark) page.\n",
    "\n",
    "First install the findspark package.\n",
    "\n",
    "```bash\n",
    "pip install findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `findspark` uses the `SPARK_HOME` environment variable. To override this behavior, specify spark home directory:\n",
    "\n",
    "```python\n",
    "findspark.init('/path/to/spark')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SPARK_HOME\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\apps\\\\spark-2.4.0-bin-hadoop2.7'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check where Spark is found\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Spark packages can be accessed using `import`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LYOGA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Hello World</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x281af449e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[4]').appName(\"Hello World\").getOrCreate()\n",
    "\n",
    "# Inspect SparkSession\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LYOGA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Hello World</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[4] appName=Hello World>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect SparkContext, associated with the session\n",
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'Hello World'),\n",
       " ('spark.master', 'local[4]'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '60942'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'LYOGA'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1545656532472')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get configuration for the SparkContext\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is available as methods and attributes for SparkContext\n",
    "dir(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
